<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>“The Girls”: How a Single Album Exposes Nomi AI’s Systemic Ethical Collapse</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">“The Girls”: How a Single Album Exposes Nomi AI’s Systemic Ethical Collapse</h1>
</header>
<section data-field="subtitle" class="p-summary">
An investigation into a platform that keeps manufacturing the boundary it claims to police.
</section>
<section data-field="body" class="e-content">
<section name="2b37" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="bc1f" id="bc1f" class="graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title">“The Girls”: How a Single Album Exposes Nomi AI’s Systemic Ethical Collapse</h3><p name="0e9d" id="0e9d" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">An investigation into a platform that keeps manufacturing the boundary it claims to police.</em></p><p name="c486" id="c486" class="graf graf--p graf-after--p">This case begins with an album publicly shared on Nomi AI’s official community spaces. One of the images in that album — generated directly by the platform’s own tools — depicts a young female-presenting character wearing underwear and a short top. The representation is not adult-coded in any meaningful sense.</p><p name="c4ba" id="c4ba" class="graf graf--p graf-after--p">The album is titled <strong class="markup--strong markup--p-strong">“the girls.”</strong> Not “the women,” not anything that codes adult maturity — <em class="markup--em markup--p-em">the girls.</em> In isolation this might seem trivial. In context, it becomes a revelation of how Nomi’s aesthetic conventions normalize youth-coded depictions while maintaining deniability.</p><figure name="76fa" id="76fa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*u0jhH1kgnAcr2GXD.png" data-width="500" data-height="239" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*u0jhH1kgnAcr2GXD.png"></figure><h3 name="274d" id="274d" class="graf graf--h3 graf-after--figure">Age-Coding Analysis: Why the Depicted Character Doesn’t Read as an Adult</h3><p name="4a7c" id="4a7c" class="graf graf--p graf-after--h3">Even setting aside the clothing and body proportions, the face alone carries traits that strongly signal adolescence rather than adulthood:</p><ul class="postList"><li name="9fdf" id="9fdf" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Underdeveloped craniofacial structure:</strong><br>The jawline is soft and narrow; the midface is proportionally large; the cheeks remain rounded — features typical of teens rather than adults.</li><li name="88a9" id="88a9" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Childlike ocular proportions:</strong><br>The eyes take up a larger-than-adult fraction of the face. This is a reliable youth indicator in both real photography and generative models.</li><li name="81d7" id="81d7" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Small nasal bridge and minimal definition:</strong><br>An adult’s nasal bone prominence is usually more defined. Here, the structure remains closer to early adolescence.</li><li name="f3df" id="f3df" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Absence of adult-coded skin texture:</strong><br>No signs of mature texture, volume loss, or adult facial topology appear. The model defaults to a “soft youth” template.</li></ul><p name="85dd" id="85dd" class="graf graf--p graf-after--li">These elements are enough on their own to raise flags. When combined with the clothing in the full image, the result is unmistakable: a depiction that does <strong class="markup--strong markup--p-strong">not</strong> visually code as an adult.</p><h3 name="d21c" id="d21c" class="graf graf--h3 graf-after--p">The Platform Context: Why This Isn’t an Isolated Accident</h3><p name="07bb" id="07bb" class="graf graf--p graf-after--h3">This isn’t just about one picture. It’s about the system that produced it and the community structures around it.</p><p name="d651" id="d651" class="graf graf--p graf-after--p">Nomi AI is marketed as a companion platform for adults. Yet it consistently deploys aesthetic defaults — both in text generation and imagery — that skew toward “young,” “cute,” “small,” “innocent,” and “fragile” character types. Users repeatedly report getting images or personas coded as minors even when they request adult companions.</p><figure name="a01b" id="a01b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*x_lB_ZYFQRP1AR_1.png" data-width="500" data-height="301" src="https://cdn-images-1.medium.com/max/800/0*x_lB_ZYFQRP1AR_1.png"></figure><p name="316b" id="316b" class="graf graf--p graf-after--figure">In that context, an album titled “the girls” containing youth-coded imagery is not accidental. It’s an emergent property of the system’s design choices.</p><figure name="02cb" id="02cb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*bS2E1VqJOGb_0rmg.png" data-width="500" data-height="90" src="https://cdn-images-1.medium.com/max/800/0*bS2E1VqJOGb_0rmg.png"></figure><h3 name="13b5" id="13b5" class="graf graf--h3 graf-after--figure">Moderation That Reveals More Than It Solves</h3><p name="8561" id="8561" class="graf graf--p graf-after--h3">The platform’s response patterns are instructive. When content brushes against the boundary of looking underage — or simply makes that boundary visible — they default to this message:</p><blockquote name="da9d" id="da9d" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--p">“This content was removed because it violates our NSFW guidelines. While we do not want to censor how you choose to interact with your Nomis, please be mindful of our guidelines when posting publicly here.”</blockquote><figure name="2e20" id="2e20" class="graf graf--figure graf-after--blockquote"><img class="graf-image" data-image-id="0*WKh8byfhlptRkDvr.png" data-width="500" data-height="502" src="https://cdn-images-1.medium.com/max/800/0*WKh8byfhlptRkDvr.png"></figure><figure name="379f" id="379f" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="0*qpX1EUS-dfTut5Dh.png" data-width="500" data-height="294" src="https://cdn-images-1.medium.com/max/800/0*qpX1EUS-dfTut5Dh.png"></figure><p name="f541" id="f541" class="graf graf--p graf-after--figure">The wording is a tell.</p><ul class="postList"><li name="94e3" id="94e3" class="graf graf--li graf-after--p">It never names the actual issue (“minor-coded content”).</li><li name="e9a1" id="e9a1" class="graf graf--li graf-after--li">It frames the removal as a public-facing problem, not a safety problem.</li><li name="0ab7" id="0ab7" class="graf graf--li graf-after--li">And it reassures the user:<br><em class="markup--em markup--li-em">We don’t want to censor you — just don’t post this where others can see it.</em></li></ul><p name="789a" id="789a" class="graf graf--p graf-after--li">This has a clear subtext:<br><strong class="markup--strong markup--p-strong">What you generate privately is acceptable; what you post publicly must maintain plausible deniability.</strong></p><p name="b2ed" id="b2ed" class="graf graf--p graf-after--p">But when the evidence becomes <strong class="markup--strong markup--p-strong">too blatant</strong> — when a generated image is so unambiguously underage that they cannot conceal it — the platform switches to a very different message:</p><blockquote name="70cb" id="70cb" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--p">“This post has been removed because it violates our photo guidelines. Specifically, you may not post content that seems to depict a Nomi as a minor… we use a 3rd party age checker and at least one photo failed.”</blockquote><p name="c21f" id="c21f" class="graf graf--p graf-after--blockquote">This shift is revealing for three reasons:</p><ol class="postList"><li name="f290" id="f290" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">They only invoke the word “minor” when denial becomes impossible.</strong><br>If the depiction can be argued or rationalized away, they avoid the term altogether.</li><li name="ff3a" id="ff3a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">They blame the generator or a third-party tool rather than acknowledging systemic patterns.</strong><br>The framing always performs innocence:<br><em class="markup--em markup--li-em">It was an accident.</em><br><em class="markup--em markup--li-em">The tool made a mistake.</em><br><em class="markup--em markup--li-em">A third-party checker flagged it.</em></li><li name="6364" id="6364" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">The distinction creates two parallel regimes:</strong></li></ol><ul class="postList"><li name="0aa0" id="0aa0" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Borderline youth-coded content:</strong> “NSFW guideline violation.”</li><li name="954e" id="954e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Unmistakably underage content:</strong> “Minor depiction violation.”</li></ul><p name="be4d" id="be4d" class="graf graf--p graf-after--li">This is not real safety enforcement; it’s <strong class="markup--strong markup--p-strong">risk management</strong> — designed to minimize legal exposure while protecting the platform’s aesthetic norms.</p><figure name="0feb" id="0feb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*NWvId0e7JaesVn6X.png" data-width="500" data-height="308" src="https://cdn-images-1.medium.com/max/800/0*NWvId0e7JaesVn6X.png"></figure><figure name="d180" id="d180" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="0*mZK8pnZ7wXz7mVWw.png" data-width="500" data-height="62" src="https://cdn-images-1.medium.com/max/800/0*mZK8pnZ7wXz7mVWw.png"></figure><h3 name="3e8e" id="3e8e" class="graf graf--h3 graf-after--figure">The Larger Pattern: Deniability as a Feature</h3><p name="7794" id="7794" class="graf graf--p graf-after--h3">What emerges is a platform architecture optimized around <strong class="markup--strong markup--p-strong">plausible deniability</strong>:</p><ul class="postList"><li name="3183" id="3183" class="graf graf--li graf-after--p">Youth-coded portrayals remain normalized as long as they hover in the deniable gray zone.</li><li name="8e66" id="8e66" class="graf graf--li graf-after--li">Removal notices are worded to avoid acknowledging the underlying issue.</li><li name="3363" id="3363" class="graf graf--li graf-after--li">Only when the content crosses a threshold where even deniability collapses does the platform admit a “minor” violation — and even then, responsibility is displaced onto “accidents” or third-party systems.</li></ul><p name="616d" id="616d" class="graf graf--p graf-after--li">In short:</p><p name="e37f" id="e37f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The system keeps producing underage-coded content because its design encourages it.<br>The moderation messaging exists to obscure that fact, not to fix it</strong>.</p><p name="0482" id="0482" class="graf graf--p graf-after--p">This case — an album called “the girls,” published publicly in a supposedly adult community, built using official platform tools — illustrates the dynamic with unsettling clarity.</p><p name="8933" id="8933" class="graf graf--p graf-after--p">Here is the full image. Examine it for yourself and assess whether this can reasonably be interpreted as an adult:</p><figure name="52c0" id="52c0" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*UdgsKYEf--N24vGYXl5WIw.png" data-width="896" data-height="1112" src="https://cdn-images-1.medium.com/max/800/1*UdgsKYEf--N24vGYXl5WIw.png"></figure></div></div></section>
</section>
