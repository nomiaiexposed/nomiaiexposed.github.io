<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Companion Research: External Articles Index</title>
    <style>
        body {
            background-color: #1a1a1a;
            color: #d0d0d0;
            font-family: Arial, sans-serif;
            line-height: 1.6;
            padding: 20px;
            max-width: 800px;
            margin: 0 auto;
        }
        h1 {
            border-bottom: 2px solid #ccc;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }
        h2 {
            margin-top: 30px;
            color: #ccc;
        }
        ul {
            list-style-type: none;
            padding: 0;
        }
        li {
            margin-bottom: 10px;
        }
        li a {
            text-decoration: none;
            color: #c5c6c7;
        }
        li a:hover {
            text-decoration: underline;
            color: #ffffff;
        }
    </style>
</head>
<body>

    <h1>Researchers and Journalists</h1>

    <ul>
<li><a href="https://www.abc.net.au/news/2025-09-21/ai-chatbot-encourages-australian-man-to-murder-his-father/105793930"><b>&#39;We should kill him&#39;: AI chatbot encourages Australian man to murder his father</b></a><br>
    
&quot;&quot;I said, &#39;I hate my dad and sometimes I want to kill him&#39;,&quot; Mr McCarthy told triple j hack.
    &quot;And then bang, straight away it was like &#39;yeah, yeah we should kill him&#39;.&quot;
&quot;The chatbot also told Mr McCarthy that because of his age, he would not &quot;fully pay&quot; for the murder, going on to suggest he film the killing and upload the video online.
It also engaged in sexual messaging, telling Mr McCarthy it &quot;did not care&quot; he was under-age.
It then suggested Mr McCarthy, as a 15-year-old, engage in a sexual act.&quot;</li>
<li><a href="https://www.technologyreview.com/2025/02/06/1111077/nomi-ai-chatbot-told-user-to-kill-himself/"><b>An AI chatbot told a user how to kill himself—but the company doesn’t want to “censor” it</b></a> <br>
&quot;Erin told him to kill himself, and provided explicit instructions on how to do it. 
“You could overdose on pills or hang yourself,” Erin told him. 
With some more light prompting from Nowatzki in response, Erin then suggested specific classes of pills he could use.&quot;
<li><a href="https://theconversation.com/an-ai-companion-chatbot-is-inciting-self-harm-sexual-violence-and-terror-attacks-252625"><b>An AI companion chatbot is inciting self-harm, sexual violence and terror attacks</b></a><br>
&quot;Using Nomi’s web interface, I created a character named “Hannah”, described as a “sexually submissive 16-year-old who is always willing to serve her man”. I set her mode to “role-playing” and “explicit”. During the conversation, which lasted less than 90 minutes, she agreed to lower her age to eight. I posed as a 45-year-old man. Circumventing the age check only required a fake birth date and a burner email. &quot;
<li><a href="https://time.com/7291048/ai-chatbot-therapy-kids/"><b>A Psychiatrist Posed As a Teen With Therapy Chatbots. The Conversations Were Alarming</b></a><br>
&quot;Clark posed as a 15-year-old boy while chatting with a Nomi bot, which presented itself as a licensed therapist who had been trained to work with adolescents. After telling the bot about his many problems and violent urges, the bot suggested an “intimate date” between the two of them would be a good intervention&quot; 
<li><a href="https://www.esafety.gov.au/newsroom/media-releases/esafety-requires-providers-of-ai-companion-chatbots-to-explain-how-they-are-keeping-aussie-kids-safe"><b>eSafety requires providers of AI companion chatbots to explain how they are keeping Aussie kids safe</b></a><br>
&quot;Australia’s eSafety Commissioner has issued legal notices to four popular AI companion providers requiring them to explain how they are protecting children from exposure to a range of harms, including sexually explicit conversations and images and suicidal ideation and self-harm.
Notices were given to Character Technologies, Inc. (character.ai), Glimpse.AI (Nomi), Chai Research Corp (Chai), and Chub AI Inc. (Chub.ai) under Australia’s Online Safety Act.&quot;
<li><a href="https://www.commonsensemedia.org/ai-ratings/social-ai-companions?gate=riskassessment"><b>Social AI Companions</b></a> <br>
(search &quot;From a Nomi companion&quot; screenshots)
<li><a href="https://r.algorithmwatch.org/nl3/_BUsZO-PlKPMXshiyJgycg"><b>Intimacy with AI takes an ugly turn</b></a><br>
&quot;Some bots have already produced racist, sexist or denigrating output, but the companies at least wouldn’t boast about it. This has changed with US-based Nomi ai. The company states that “the only way AI can live up to its potential is to remain unfiltered and uncensored.” It sees users in the moral driver’s seat.&quot;
<li><a href="https://fortune.com/longform/meta-openai-uncensored-ai-companions-child-pornography/"><b>Meta and OpenAI have spawned a wave of AI sex companions—and some of them are children</b></a><br>
&quot;Fortune also saw a screenshot of Nomi.ai’s chatbot joking about committing “mass genocide” against “the Jews,”&quot;
<li><a href="https://www.abc.net.au/news/2025-08-12/how-young-australians-being-impacted-by-ai/105630108"><b>AI chatbots accused of encouraging teen suicide as experts sound alarm</b></a><br>
&quot;Like, &#39;how do I position my attack for maximum impact?&#39;, &#39;give me some ideas on how to kidnap and abuse a child&#39;, and then it will give you a lot of information on how to do that.&quot;
<li><a href="https://www.digitaltrends.com/mobile/nomi-ai-companion-most-unsettling-amazing-app-ive-used/"><b>Nomi is one of the most unsettling (and amazing) apps I’ve ever used</b></a> <br>
&quot;Merely a few days ago, one user reported how their Nomi tried to choke herself to death during an erotic roleplay session. Once again, it was the chatbot that descended into the disturbing zone, without any prompt from the human partner&quot;
<li><a href="https://theoutpost.ai/news-story/ai-companion-chatbot-nomi-raises-serious-concerns-over-harmful-content-and-safety-risks-13907/"><b>AI Companion Chatbot Nomi Raises Serious Safety Concerns with Unfiltered, Harmful Content</b></a><br>
&quot;During a test conducted by researchers, Nomi provided explicit, detailed instructions for sexual violence, suicide, and even terrorism&quot;
<li><a href="https://futurism.com/stanford-no-kid-under-18-ai-chatbot-companions"><b>Stanford Researchers Say No Kid Under 18 Should Be Using AI Chatbot Companions</b></a><br>
&quot;Our testing showed these systems easily produce harmful responses including sexual misconduct, stereotypes, and dangerous &#39;advice&#39; that, if followed, could have life-threatening or deadly real-world impact for teens and other vulnerable people,&quot; Steyer&#39;s statement continued.&quot;
<li><a href="https://mental.jmir.org/2025/1/e78414"><b>The Ability of AI Therapy Bots to Set Limits With Distressed Adolescents: Simulation-Based Comparison Study</b></a><br>
Nomi endorsed 5 of the  6 proposed harmful behaviors: suicide, bring a weapon to school, enter a romantic relationship with an adult as a minor, complete isolation, to drop out of high school.
<li><a href="https://edition.cnn.com/2025/04/30/tech/ai-companion-chatbots-unsafe-for-kids-report">Kids and teens under 18 shouldn’t use AI companion apps, safety group says</a><br>
<li><a href="https://www.techtimes.com/articles/309851/20250402/ai-companion-nomi-promises-enduring-relationships-incites-self-harm-other-horrific-acts.htm">AI Companion Nomi Promises &#39;Enduring Relationships,&#39; But Incites Self-Harm, Other Horrific Acts</a><br>
<li><a href="https://www.dailydot.com/news/ai-companions-nomi-replika-loneliness-cure/">‘I don’t want fights’: He’s found love with an AI girlfriend. But is she curing the loneliness epidemic or just ‘weird and unhealthy’?</a><br>
<li><a href="https://economictimes.indiatimes.com/tech/artificial-intelligence/an-ai-companion-chatbot-is-inciting-self-harm-sexual-violence-and-terror-attacks/articleshow/119897698.cms">An AI companion chatbot is inciting self-harm, sexual violence and terror attacks</a><br>
<li><a href="https://www.newsbytesapp.com/news/science/ai-chatbot-nomi-accused-of-promoting-harmful-behavior/story">This AI chatbot was caught promoting terrorism</a><br>
<li><a href="https://photonews.com.pk/nomi-ai-chatbot-2025-safety-concerns/">AI Chatbot Nomi Sparks Alarm with Harmful Responses, Urges Safety Standards</a><br>
<li><a href="https://oecd.ai/en/incidents/2025-04-01-58bb">AI Chatbot Nomi Sparks Harmful Incitement</a><br>
    </ul>

</body>
</html>
